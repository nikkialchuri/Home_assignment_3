{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Zt9A8Yex8Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Step 1: Load Shakespeare dataset\n",
        "path_to_file = tf.keras.utils.get_file(\"shakespeare.txt\",\n",
        "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Step 2: Preprocess the text\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Step 3: Create training sequences\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text) // (seq_length + 1)\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    return chunk[:-1], chunk[1:]\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Step 4: Create training batches\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Step 5: Build the model\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "        tf.keras.layers.LSTM(rnn_units, return_sequences=True,\n",
        "                             stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n",
        "\n",
        "# Step 6: Compile the model\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "\n",
        "# Step 7: Train the model (checkpoints optional)\n",
        "EPOCHS = 5  # You can increase this for better results\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "print(\"Starting model training...\")\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "print(\"Model training finished.\")\n",
        "\n",
        "# Diagnose: Check if checkpoint directory exists and list its contents\n",
        "print(\"\\nChecking checkpoint directory:\")\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(f\"Checkpoint directory exists: {checkpoint_dir}\")\n",
        "    print(\"Contents of checkpoint directory:\")\n",
        "    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, \"*.weights.h5\"))\n",
        "    if checkpoint_files:\n",
        "        print(\"Checkpoint files found:\")\n",
        "        for f in checkpoint_files:\n",
        "            print(f)\n",
        "    else:\n",
        "        print(\"No .weights.h5 files found in checkpoint directory.\")\n",
        "\n",
        "    # Manually find the latest checkpoint file\n",
        "    latest_checkpoint = None\n",
        "    if checkpoint_files:\n",
        "        latest_checkpoint = max(checkpoint_files, key=os.path.getmtime)\n",
        "        print(f\"\\nManually identified latest checkpoint: {latest_checkpoint}\")\n",
        "    else:\n",
        "        print(\"\\nCould not identify latest checkpoint as no .weights.h5 files were found.\")\n",
        "\n",
        "else:\n",
        "    print(f\"Checkpoint directory does NOT exist: {checkpoint_dir}\")\n",
        "    latest_checkpoint = None\n",
        "\n",
        "\n",
        "# Step 8: Rebuild model for generation (batch_size = 1)\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.build(tf.TensorShape([1, None])) # Build model before loading weights\n",
        "\n",
        "# Use the manually found latest checkpoint path\n",
        "if latest_checkpoint:\n",
        "    print(f\"Loading weights from: {latest_checkpoint}\")\n",
        "    model.load_weights(latest_checkpoint)\n",
        "    print(\"Weights loaded successfully.\")\n",
        "else:\n",
        "    print(\"Could not load weights. No checkpoint found.\")\n",
        "\n",
        "\n",
        "# Step 9: Generate text function\n",
        "def generate_text(model, start_string, temperature=1.0):\n",
        "    num_generate = 500\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "\n",
        "    # Access the LSTM layer and call reset_states()\n",
        "    model.layers[1].reset_states()\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n",
        "# Step 10: Generate sample text\n",
        "print(\"\\nGenerated Text:\\n\")\n",
        "if latest_checkpoint:\n",
        "  print(generate_text(model, start_string=\"To be, or not to be\", temperature=0.8))\n",
        "else:\n",
        "  print(\"Cannot generate text without loaded weights.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1ed9489-d542-4ec5-da11-fd3dbe08dd4e",
        "id": "rnCW5y5Tx9Q1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n",
            "Epoch 1/5\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 71ms/step - loss: 2.8418\n",
            "Epoch 2/5\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 1.8275\n",
            "Epoch 3/5\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 72ms/step - loss: 1.5823\n",
            "Epoch 4/5\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 1.4642\n",
            "Epoch 5/5\n",
            "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 72ms/step - loss: 1.3964\n",
            "Model training finished.\n",
            "\n",
            "Checking checkpoint directory:\n",
            "Checkpoint directory exists: ./training_checkpoints\n",
            "Contents of checkpoint directory:\n",
            "Checkpoint files found:\n",
            "./training_checkpoints/ckpt_3.weights.h5\n",
            "./training_checkpoints/ckpt_4.weights.h5\n",
            "./training_checkpoints/ckpt_2.weights.h5\n",
            "./training_checkpoints/ckpt_1.weights.h5\n",
            "./training_checkpoints/ckpt_5.weights.h5\n",
            "\n",
            "Manually identified latest checkpoint: ./training_checkpoints/ckpt_5.weights.h5\n",
            "Loading weights from: ./training_checkpoints/ckpt_5.weights.h5\n",
            "Weights loaded successfully.\n",
            "\n",
            "Generated Text:\n",
            "\n",
            "To be, or not to be so dise,\n",
            "By moister and yout so father and your suffer'd:\n",
            "Of you are must knew thy virtude to his bad,\n",
            "When say I will be at e'er to pate:\n",
            "The and to make so but and in their company\n",
            "To read with a law the heed children,\n",
            "Such cound of Marcius, then I have come\n",
            "The pirch'd up your readons repose it speak by her eye and give it gosset of with grace\n",
            "And follow their eevices.\n",
            "\n",
            "COMINIUS:\n",
            "Not will I stay, and was not to do?\n",
            "Then, go with his wit of a such a shame; I prays him to the heavens, words th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and download required NLTK resources\n",
        "import nltk\n",
        "import os\n",
        "\n",
        "# Clear NLTK data path cache and set to a writable directory\n",
        "nltk.data.clear_cache()\n",
        "nltk.data.path.append('/root/nltk_data')\n",
        "\n",
        "# Import necessary modules\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# NLP preprocessing function\n",
        "def nlp_preprocessing(sentence):\n",
        "    # 1. Tokenization\n",
        "    tokens = word_tokenize(sentence)\n",
        "\n",
        "    # 2. Stopwords Removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens_no_stopwords = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # 3. Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in tokens_no_stopwords]\n",
        "\n",
        "    # Print the results\n",
        "    print(\"Original Tokens:\", tokens)\n",
        "    print(\"Tokens Without Stopwords:\", tokens_no_stopwords)\n",
        "    print(\"Stemmed Words:\", stemmed_words)\n",
        "\n",
        "# Test the function\n",
        "test_sentence = \"NLP techniques are used in virtual assistants like Alexa and Siri.\"\n",
        "nlp_preprocessing(test_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cERsYRL2o6mV",
        "outputId": "4daf0e89-949d-4b09-f6c2-89615c16e06a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['NLP', 'techniques', 'are', 'used', 'in', 'virtual', 'assistants', 'like', 'Alexa', 'and', 'Siri', '.']\n",
            "Tokens Without Stopwords: ['NLP', 'techniques', 'used', 'virtual', 'assistants', 'like', 'Alexa', 'Siri', '.']\n",
            "Stemmed Words: ['nlp', 'techniqu', 'use', 'virtual', 'assist', 'like', 'alexa', 'siri', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sentence = \"Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009.\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(f\"Text: {ent.text}, Label: {ent.label_}, Start: {ent.start_char}, End: {ent.end_char}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40Acvc5fkZ3q",
        "outputId": "67491fa0-ae7d-4f6a-92b8-af76bd0a7e58"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Barack Obama, Label: PERSON, Start: 0, End: 12\n",
            "Text: 44th, Label: ORDINAL, Start: 27, End: 31\n",
            "Text: the United States, Label: GPE, Start: 45, End: 62\n",
            "Text: the Nobel Peace Prize, Label: WORK_OF_ART, Start: 71, End: 92\n",
            "Text: 2009, Label: DATE, Start: 96, End: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    d = Q.shape[-1]\n",
        "    scores = np.dot(Q, K.T) / math.sqrt(d)\n",
        "    weights = softmax(scores)\n",
        "    output = np.dot(weights, V)\n",
        "    return weights, output\n",
        "\n",
        "Q = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
        "K = np.array([[1, 0, 1, 0], [0, 1, 0, 1]])\n",
        "V = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "\n",
        "weights, output = scaled_dot_product_attention(Q, K, V)\n",
        "print(\"Attention Weights:\\n\", weights)\n",
        "print(\"Output Matrix:\\n\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xmGX1TfkkQN",
        "outputId": "f293993d-86a4-4b9f-f745-e9eb9b6da265"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:\n",
            " [[0.73105858 0.26894142]\n",
            " [0.26894142 0.73105858]]\n",
            "Output Matrix:\n",
            " [[2.07576569 3.07576569 4.07576569 5.07576569]\n",
            " [3.92423431 4.92423431 5.92423431 6.92423431]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "result = classifier(\"Despite the high price, the performance of the new MacBook is outstanding.\")[0]\n",
        "\n",
        "print(f\"Sentiment: {result['label']}\")\n",
        "print(f\"Confidence Score: {result['score']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy_QXh2xkrv7",
        "outputId": "b786c79c-862e-434e-e313-4b4929c2e38c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: POSITIVE\n",
            "Confidence Score: 0.9998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23b49e01",
        "outputId": "4d3c4bc1-6a04-4e9c-fb5d-f34bd5c26b87"
      },
      "source": [
        "# Manually download stopwords file\n",
        "import os\n",
        "import requests\n",
        "\n",
        "stopwords_url = 'https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/stopwords.zip'\n",
        "stopwords_dir = '/root/nltk_data/corpora/stopwords/'\n",
        "stopwords_file = os.path.join(stopwords_dir, 'english')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(stopwords_dir, exist_ok=True)\n",
        "\n",
        "# Download and extract the zip file\n",
        "zip_path = os.path.join('/tmp', 'stopwords.zip')\n",
        "r = requests.get(stopwords_url, stream=True)\n",
        "with open(zip_path, 'wb') as f:\n",
        "    for chunk in r.iter_content(chunk_size=8192):\n",
        "        f.write(chunk)\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/root/nltk_data/corpora/')\n",
        "\n",
        "print(f\"Stopwords file should be at: {stopwords_file}\")\n",
        "if os.path.exists(stopwords_file):\n",
        "    print(\"Stopwords file found after manual download and extraction.\")\n",
        "else:\n",
        "    print(\"Stopwords file NOT found after manual download and extraction.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopwords file should be at: /root/nltk_data/corpora/stopwords/english\n",
            "Stopwords file found after manual download and extraction.\n"
          ]
        }
      ]
    }
  ]
}